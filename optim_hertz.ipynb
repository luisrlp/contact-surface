{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization of Hertz NN regression model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Math and Dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Others\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_fonts = {\n",
    "    # Use LaTeX to write all text\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    # Use 10pt font in plots, to match 10pt font in document\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"font.size\": 12,\n",
    "    # Make the legend/label fonts a little smaller\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10\n",
    "}\n",
    "\n",
    "# update to latex fonts\n",
    "plt.rcParams.update(tex_fonts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hertz(i, E, nu, r):\n",
    "    a = i/r\n",
    "    factor = 1 - 0.1 * a - (1/840) * a**2 + (11/15120) * a**3 + (1357/6652800) * a**4\n",
    "    force = 4/3 * E / (1 - nu**2) * np.sqrt(r)*i**1.5 * factor\n",
    "    force[np.isnan(force)] = 0\n",
    "    return force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution of the map\n",
    "res = 100\n",
    "# random values\n",
    "size = res * res\n",
    "# Seed (if needed)\n",
    "np.random.seed(42)\n",
    "# Young's modulus [kPa] - random values following a normal distribution\n",
    "    #loc: mean/center of distribution\n",
    "    #scale: std\n",
    "E = abs(np.random.normal(loc=1.0, scale=0.3, size=size)) # !!!!!!!!!!! colocar o abs\n",
    "# Poisson's ratio \n",
    "nu = 0.5\n",
    "# radius of the indenter\n",
    "r = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no contact approach. less points\n",
    "#linspace(p1, p2, n_pts)\n",
    "no_contact = np.linspace(-10, 0, 3)\n",
    "\n",
    "'''DISPLACEMENT VECTORS'''\n",
    "xmin, xmax, npts = 0, 4, 20\n",
    "\n",
    "'''Uniformly distributed disp. vectors'''\n",
    "# indentation depth. more points\n",
    "contact = np.linspace(xmin, xmax, npts)\n",
    "# approach and withdraw\n",
    "approach = np.concatenate([no_contact[:-1], contact])\n",
    "withdraw = np.flip(approach)\n",
    "ramp = np.concatenate([approach, withdraw])\n",
    "\n",
    "'''Randomly distributed disp. vectors'''\n",
    "# Seed (if needed)\n",
    "np.random.seed(42)\n",
    "\n",
    "rnd_contact_list = [contact]\n",
    "for _ in range(size-1):\n",
    "    aux = np.random.random(npts).cumsum()\n",
    "    aux = (aux-aux.min()) / aux.ptp()     #... .ptp(): peak to peak, i.e., xmax-xmin\n",
    "    aux = (xmax-xmin)*aux + xmin\n",
    "    rnd_contact_list.append(aux)\n",
    "rnd_contact = np.array(rnd_contact_list)\n",
    "rnd_approach = np.concatenate([np.repeat([no_contact[:-1]], size, axis=0), rnd_contact], axis=1)\n",
    "rnd_withdraw = np.flip(rnd_approach, axis=1)\n",
    "\n",
    "# define ramp time\n",
    "half_cycle = 2 \n",
    "t_approach = half_cycle*((approach - approach.min(axis=0)) / (approach.max(axis=0) - approach.min(axis=0)))\n",
    "t_withdraw = half_cycle*((withdraw - withdraw.max(axis=0)) / (withdraw.min(axis=0) - withdraw.max(axis=0)))+max(t_approach)\n",
    "t = np.concatenate([t_approach, t_withdraw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisr\\AppData\\Local\\Temp\\ipykernel_9576\\2420640097.py:4: RuntimeWarning: invalid value encountered in power\n",
      "  force = 4/3 * E / (1 - nu**2) * np.sqrt(r)*i**1.5 * factor\n"
     ]
    }
   ],
   "source": [
    "# construct dataframe\n",
    "df = pd.DataFrame()\n",
    "# 'E' and 'gamma' arrays to list:\n",
    "df['E'] = E.tolist()\n",
    "# assigns the displacement array for each 'E' (num of E values = len(df) = size)\n",
    "df['approach'] = [rnd_approach[app] for app in range(len(df))]\n",
    "# applies hertz and jkr models to each row (axis= 0(col) or 1(row))\n",
    "    # x will take the values of each row \n",
    "df['f_hertz'] = df.apply(lambda x: hertz(x.approach, x.E, nu, r), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) (20,)\n"
     ]
    }
   ],
   "source": [
    "#dataframe with contact-only data\n",
    "#df_hc: hertz contact\n",
    "df_hc = pd.DataFrame()\n",
    "df_hc['approach_contact'] = df['approach'].apply(lambda x: x[x>=0])\n",
    "df_hc['f_hertz_contact'] = df['f_hertz'].apply(lambda x: x[len(no_contact)-1:])\n",
    "df_hc['E_hertz'] = df['E']\n",
    "#df_hc['appproach_contact'] = df.apply(lambda x: x.approach[x.approach>=0], axis=1)\n",
    "#check size of disp and force vectors\n",
    "print(df_hc['approach_contact'][0].shape, df_hc['f_hertz_contact'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hc = np.array(df_hc[['approach_contact', 'f_hertz_contact']])\n",
    "y_hc = np.array(df_hc['E_hertz'])\n",
    "\n",
    "test_ratio = 0.15\n",
    "# (!!!) validation ratio is currently given in relation to the entire dataset (!!!!)\n",
    "valid_ratio = 0.15 \n",
    "rnd_state = 42\n",
    "\n",
    "# Without stratify\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_hc, y_hc, test_size=test_ratio, random_state=rnd_state)\n",
    "\n",
    "#With stratify\n",
    "bin_count = 50\n",
    "bins = pd.qcut(y_hc, bin_count, labels=False, duplicates='drop')\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_hc, y_hc, test_size=test_ratio,\n",
    "                                                     random_state=rnd_state, stratify = bins)\n",
    "\n",
    "bins = pd.qcut(y_train, bin_count, labels=False, duplicates='drop')\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=valid_ratio/(1-test_ratio),\n",
    "                                                       random_state=rnd_state, stratify = bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test data from np arrays to torch tensor with desired shape\n",
    "def tensor_input_shape(nparray):\n",
    "    '''\n",
    "    Input: nparray - numpy array with two dimensions (n_samples, n_features)\n",
    "    Output: torch_tensor - pytorch tensor with 3 dimensions (n_samples, n_pts, n_features) \n",
    "    '''\n",
    "    n_samples = len(nparray)\n",
    "    n_pts = len(nparray[0,0])\n",
    "    torch_tensor = torch.zeros(size=(n_samples, n_pts, 2))\n",
    "    for i in range(n_samples):\n",
    "        aux_nparray = np.hstack((nparray[i,0].reshape((n_pts,1)), nparray[i,1].reshape((n_pts,1))))\n",
    "        aux_ttensor = torch.from_numpy(aux_nparray).type(torch.float)\n",
    "        torch_tensor[i,:,:] = aux_ttensor\n",
    "    return torch_tensor\n",
    "\n",
    "x_train_t = tensor_input_shape(x_train)\n",
    "x_valid_t = tensor_input_shape(x_valid)\n",
    "x_test_t = tensor_input_shape(x_test)\n",
    "y_train_t = torch.from_numpy(y_train).type(torch.float).unsqueeze(dim=1)\n",
    "y_valid_t = torch.from_numpy(y_valid).type(torch.float).unsqueeze(dim=1)\n",
    "y_test_t = torch.from_numpy(y_test).type(torch.float).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dir(timestamp, contact_model: str):\n",
    "\n",
    "  ''' Second input must be 'hertz' or 'jkr' '''\n",
    "  \n",
    "  allowed_models = ['hertz', 'jkr']\n",
    "  if contact_model not in allowed_models:\n",
    "    raise ValueError(\"Input value must be one of %s\" % allowed_models)\n",
    "  model_path = 'model_{}'.format(timestamp)\n",
    "  parent_dir = 'c:\\\\Users\\\\luisr\\\\OneDrive\\\\Ambiente de Trabalho\\\\Tese'\n",
    "  if contact_model == 'hertz':\n",
    "    dir = 'Hertz_models'\n",
    "  elif contact_model == 'jkr':\n",
    "    dir = 'JKR_models'\n",
    "  path = os.path.join(parent_dir, dir, model_path)\n",
    "  # path = os.path.join(initial_wd, dir, model_path)\n",
    "  os.mkdir(path)\n",
    "  os.chdir(path)\n",
    "\n",
    "def error_fn(predict_tensor, label_tensor):\n",
    "  '''\n",
    "  INPUTS: * two tensors - true labels and predicts\n",
    "  OUTPUTS: * scalar - mean relative error (in %) between both tensors\n",
    "           * list - relative error (%) for each prediction\n",
    "  '''\n",
    "  error = abs((label_tensor-predict_tensor)/label_tensor*100).squeeze(dim=1).mean().item()\n",
    "  error_list = list(abs((label_tensor-predict_tensor)/label_tensor*100).squeeze(dim=1).detach().numpy())\n",
    "  return error, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hertz_Dataset():\n",
    "  \n",
    "  def __init__(self,features,labels):\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.features[idx],self.labels[idx]\n",
    "  \n",
    "train_data = Hertz_Dataset(x_train_t, y_train_t)\n",
    "test_data = Hertz_Dataset(x_test_t, y_test_t)\n",
    "valid_data = Hertz_Dataset(x_valid_t, y_valid_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ After changing one of the hyperparameters: ########################\n",
    "### Re-run the cells where the model class and the model_params dict are defined ###\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Size of each layer\n",
    "HIDDEN_UNITS_1 = 512\n",
    "HIDDEN_UNITS_2 = 256\n",
    "\n",
    "ARCHITECTURE = 1\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader=DataLoader(test_data,batch_size=int(test_ratio*size+1),shuffle=False)\n",
    "valid_loader=DataLoader(valid_data, batch_size=int(valid_ratio*size+1), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear regression model\n",
    "class Regression_Hertz(nn.Module):\n",
    "    def __init__(self, input_shape, HIDDEN_UNITS_1, HIDDEN_UNITS_2):\n",
    "        super(Regression_Hertz, self).__init__()\n",
    "        input_size = input_shape[0] * input_shape[1]\n",
    "        self.layers = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(input_size, HIDDEN_UNITS_1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_1,HIDDEN_UNITS_2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_2, 1))\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "# Define input shape\n",
    "input_shape = x_train_t.shape[1:]\n",
    "\n",
    "# Instantiate the model\n",
    "torch.manual_seed(42)\n",
    "model_Hertz = Regression_Hertz(input_shape, HIDDEN_UNITS_1, HIDDEN_UNITS_2).to(DEVICE)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, train_loader, optimizer): # (epoch_index, tb_writer)\n",
    "    # running_loss = 0.\n",
    "    # last_loss = 0.\n",
    "    loss_list = []\n",
    "    error_list = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_Hertz(inputs.to(DEVICE))\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels.to(DEVICE))\n",
    "        # error, _ = error_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        # Gather data and report\n",
    "        loss_list.append(loss.item())\n",
    "        # error_list.append(error)\n",
    "        # running_loss += loss.item()  # .item() converts tensor to number\n",
    "        # print(i, loss.item())\n",
    "    return loss_list, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-18 16:55:19,916]\u001b[0m A new study created in memory with name: no-name-94da5c1c-05d9-40ec-961e-8b77ae957183\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:55:38,733]\u001b[0m Trial 0 finished with value: 0.0004005391092505306 and parameters: {'learning_rate': 0.0004327345554616446, 'n_epochs': 40, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.0004005391092505306.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:55:54,172]\u001b[0m Trial 1 finished with value: 3.607854887377471e-05 and parameters: {'learning_rate': 0.008603419877514803, 'n_epochs': 32, 'optimizer': 'Adam'}. Best is trial 1 with value: 3.607854887377471e-05.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:56:22,886]\u001b[0m Trial 2 finished with value: 9.915846021613106e-06 and parameters: {'learning_rate': 0.015577393228400509, 'n_epochs': 65, 'optimizer': 'SGD'}. Best is trial 2 with value: 9.915846021613106e-06.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:56:32,419]\u001b[0m Trial 3 finished with value: 1.6430456525995396e-05 and parameters: {'learning_rate': 0.022772653431494042, 'n_epochs': 20, 'optimizer': 'SGD'}. Best is trial 2 with value: 9.915846021613106e-06.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:57:00,247]\u001b[0m Trial 4 finished with value: 1.0662308341125026e-05 and parameters: {'learning_rate': 2.383339166372559e-05, 'n_epochs': 61, 'optimizer': 'SGD'}. Best is trial 2 with value: 9.915846021613106e-06.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:57:43,442]\u001b[0m Trial 5 finished with value: 8.322683129335928e-07 and parameters: {'learning_rate': 0.00013250871509140283, 'n_epochs': 89, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:58:25,710]\u001b[0m Trial 6 finished with value: 0.0912996381521225 and parameters: {'learning_rate': 0.04660954533490066, 'n_epochs': 86, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:59:04,064]\u001b[0m Trial 7 finished with value: 0.09123044461011887 and parameters: {'learning_rate': 2.0834936380173126e-06, 'n_epochs': 78, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 16:59:47,402]\u001b[0m Trial 8 finished with value: 0.09120988845825195 and parameters: {'learning_rate': 0.0004561482374927256, 'n_epochs': 90, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:00:10,968]\u001b[0m Trial 9 finished with value: 0.09120626747608185 and parameters: {'learning_rate': 0.008719911836043575, 'n_epochs': 48, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:01:01,626]\u001b[0m Trial 10 finished with value: 0.09120383858680725 and parameters: {'learning_rate': 3.730574765965722e-05, 'n_epochs': 100, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:01:31,264]\u001b[0m Trial 11 finished with value: 0.09120430052280426 and parameters: {'learning_rate': 0.0016024752320253242, 'n_epochs': 66, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:02:03,937]\u001b[0m Trial 12 finished with value: 0.09120390564203262 and parameters: {'learning_rate': 4.894364455709197e-05, 'n_epochs': 71, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:02:51,282]\u001b[0m Trial 13 finished with value: 0.09120438247919083 and parameters: {'learning_rate': 0.0016830003127476885, 'n_epochs': 100, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:03:16,802]\u001b[0m Trial 14 finished with value: 0.09120414406061172 and parameters: {'learning_rate': 1.927380329269298e-06, 'n_epochs': 53, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:03:52,152]\u001b[0m Trial 15 finished with value: 0.09120385348796844 and parameters: {'learning_rate': 0.00012925010685767537, 'n_epochs': 80, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:04:26,816]\u001b[0m Trial 16 finished with value: 0.09148433059453964 and parameters: {'learning_rate': 0.08200972192334857, 'n_epochs': 72, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:05:14,162]\u001b[0m Trial 17 finished with value: 0.09120418131351471 and parameters: {'learning_rate': 8.188921272892363e-06, 'n_epochs': 89, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:05:45,067]\u001b[0m Trial 18 finished with value: 0.09120385348796844 and parameters: {'learning_rate': 0.0017984654822446376, 'n_epochs': 60, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:05:51,259]\u001b[0m Trial 19 finished with value: 0.09120379388332367 and parameters: {'learning_rate': 0.00023616809540297643, 'n_epochs': 13, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:06:14,240]\u001b[0m Trial 20 finished with value: 0.09122990816831589 and parameters: {'learning_rate': 0.005595555241971661, 'n_epochs': 46, 'optimizer': 'Adam'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:06:43,727]\u001b[0m Trial 21 finished with value: 0.09121653437614441 and parameters: {'learning_rate': 1.2759219872259696e-05, 'n_epochs': 61, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:07:02,603]\u001b[0m Trial 22 finished with value: 0.09120593219995499 and parameters: {'learning_rate': 5.598920488122549e-05, 'n_epochs': 33, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:07:34,113]\u001b[0m Trial 23 finished with value: 0.09120509028434753 and parameters: {'learning_rate': 8.89841773482254e-06, 'n_epochs': 55, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:08:16,858]\u001b[0m Trial 24 finished with value: 0.09120383858680725 and parameters: {'learning_rate': 0.00012353557472603467, 'n_epochs': 77, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[32m[I 2023-04-18 17:08:47,166]\u001b[0m Trial 25 finished with value: 0.09120383858680725 and parameters: {'learning_rate': 2.5675157627561337e-05, 'n_epochs': 60, 'optimizer': 'SGD'}. Best is trial 5 with value: 8.322683129335928e-07.\u001b[0m\n",
      "\u001b[33m[W 2023-04-18 17:08:47,400]\u001b[0m Trial 26 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\luisr\\AppData\\Local\\Temp\\ipykernel_9576\\1566519327.py\", line 28, in objective\n",
      "    optimizer.step()\n",
      "  File \"c:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 269, in wrapper\n",
      "    with torch.autograd.profiler.record_function(profile_name):\n",
      "  File \"c:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\torch\\autograd\\profiler.py\", line 492, in __enter__\n",
      "    self.record = torch.ops.profiler._record_function_enter_new(self.name, self.args)\n",
      "  File \"c:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\torch\\_ops.py\", line 502, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     57\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n\u001b[0;32m     60\u001b[0m     pruned_trials \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mget_trials(deepcopy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, states\u001b[39m=\u001b[39m[TrialState\u001b[39m.\u001b[39mPRUNED])\n\u001b[0;32m     61\u001b[0m     complete_trials \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mget_trials(deepcopy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, states\u001b[39m=\u001b[39m[TrialState\u001b[39m.\u001b[39mCOMPLETE])\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     _optimize(\n\u001b[0;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[214], line 28\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     26\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     27\u001b[0m     \u001b[39m# Adjust learning weights\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     30\u001b[0m \u001b[39m# Evaluation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model_Hertz\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:269\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[0;32m    268\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m    270\u001b[0m     \u001b[39m# call optimizer step pre hooks\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[39mfor\u001b[39;00m pre_hook \u001b[39min\u001b[39;00m chain(_global_optimizer_pre_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_pre_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    272\u001b[0m         result \u001b[39m=\u001b[39m pre_hook(\u001b[39mself\u001b[39m, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\torch\\autograd\\profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecord \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[0;32m    493\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luisr\\OneDrive\\Ambiente de Trabalho\\Tese\\venv\\lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs \u001b[39mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n",
    "    n_epochs = trial.suggest_int('n_epochs', 10, 100)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    \n",
    "    # n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    # n_nodes = trial.suggest_int(\"n_nodes\", 4, 64, log=True)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = getattr(optim, optimizer_name)(model_Hertz.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model_Hertz.train(True)\n",
    "        loss_list = []\n",
    "        error_list = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_Hertz(inputs.to(DEVICE))\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, labels.to(DEVICE))\n",
    "            # error, _ = error_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model_Hertz.eval()\n",
    "        running_vloss = 0.0\n",
    "        running_verror = 0.0\n",
    "        verror_list, fts_list, labels_list, predicts_list = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(valid_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model_Hertz(vinputs.to(DEVICE))\n",
    "                predicts_list.append(voutputs)\n",
    "                vloss = loss_fn(voutputs, vlabels.to(DEVICE))\n",
    "                # verror, verror_aux_list = error_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "                # running_verror += verror\n",
    "                # verror_list += verror_aux_list\n",
    "            loss = running_vloss / (i + 1)\n",
    "            # print(loss.item())\n",
    "            # avg_verror = running_verror / (i + 1)\n",
    "            # print(avg_verror)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=500)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
