{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization of JKR NN regression model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Math and Dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Machine Learning \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Others\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hertz(i, E, nu, r):\n",
    "    a = i/r\n",
    "    factor = 1 - 0.1 * a - (1/840) * a**2 + (11/15120) * a**3 + (1357/6652800) * a**4\n",
    "    force = 4/3 * E / (1 - nu**2) * np.sqrt(r)*i**1.5 * factor\n",
    "    # make nan values zero\n",
    "    force[np.isnan(force)] = 0\n",
    "    return force*10**-6\n",
    "\n",
    "def jkr(i, E, nu, gamma, r):\n",
    "    E = E * 10**3 # kPa to Pa\n",
    "    i = i * 10**-9 # nm to m\n",
    "    r = r * 10**-9 # nm to m\n",
    "    gamma = gamma * 10**-6 # microJ/m^2 to J/m^2\n",
    "    E_eff = E / (1 - nu**2)\n",
    "    K = 4/3 * E_eff\n",
    "    Ua = np.sqrt(6*np.pi*gamma)\n",
    "    # JKR force formula in (3)\n",
    "    force = K * r **0.5 * i**1.5 - Ua * K**0.5 * r**0.75 * i**0.75\n",
    "    # make nan values zero\n",
    "    force[np.isnan(force)] = 0\n",
    "    # return force*10**-6\n",
    "    return force*10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolution of the map\n",
    "res = 200\n",
    "# random values\n",
    "size = res * res\n",
    "size = 10_000\n",
    "# Seed (if needed)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Young's modulus [kPa] - random values following a normal distribution\n",
    "    #loc: mean/center of distribution\n",
    "    #scale: std\n",
    "# Normal distribution: \n",
    "'''E = abs(np.random.normal(loc=2.5, scale=1.8, size=size))\n",
    "# Define a minimum value for a normal distribution, by multiplying lower values by a factor\n",
    "while np.any(E < 0.2):\n",
    "    E = np.where(E < 0.2, E*10, E)'''\n",
    "\n",
    "# Uniform distribution:\n",
    "# Uniform distribution with random seed\n",
    "# np.random.seed(42)\n",
    "# E = abs(np.random.uniform(low=0.2, high=10., size=size))\n",
    "\n",
    "# Triangular distrbution\n",
    "E = np.random.triangular(left=0.2, mode=1.8, right=10, size=size)\n",
    "\n",
    "# Poisson's ratio \n",
    "nu = 0.5\n",
    "# surface energy\n",
    "    #gamma in (3) is given in J sub-units(???), but gamma <> Ua\n",
    "# gamma = abs(np.random.normal(loc=0.0001, scale=0.000003, size=size)) f (0,40)\n",
    "# gamma = abs(np.random.normal(loc=2, scale=0.28, size=size))\n",
    "gamma = abs(np.random.uniform(low=1, high=3, size=size))\n",
    "# gamma = 0.1\n",
    "# radius of the indenter\n",
    "r = 1980.0 # (nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no contact approach. less points\n",
    "#linspace(p1, p2, n_pts)\n",
    "no_contact = np.linspace(-800, 0, 3)\n",
    "\n",
    "'''DISPLACEMENT VECTORS'''\n",
    "# xmin, xmax, npts = 0, 250, 50\n",
    "xmin, xmax, npts = 0, 150, 50\n",
    "\n",
    "'''Uniformly distributed disp. vectors'''\n",
    "# indentation depth. more points\n",
    "contact = np.linspace(xmin, xmax, npts+1)\n",
    "# approach and withdraw\n",
    "approach = np.concatenate([no_contact[:-1], contact])\n",
    "withdraw = np.flip(approach)\n",
    "ramp = np.concatenate([approach, withdraw])\n",
    "\n",
    "'''Randomly distributed disp. vectors'''\n",
    "# Seed (if needed)\n",
    "np.random.seed(42)\n",
    "\n",
    "rnd_contact_list = [contact]\n",
    "for _ in range(size-1):\n",
    "    aux = np.random.random(npts+1).cumsum()\n",
    "    aux = (aux-aux.min()) / aux.ptp()     #... .ptp(): peak to peak, i.e., xmax-xmin\n",
    "    aux = (xmax-xmin)*aux + xmin\n",
    "    rnd_contact_list.append(aux)\n",
    "rnd_contact = np.array(rnd_contact_list)\n",
    "rnd_approach = np.concatenate([np.repeat([no_contact[:-1]], size, axis=0), rnd_contact], axis=1)\n",
    "rnd_withdraw = np.flip(rnd_approach, axis=1)\n",
    "\n",
    "# define ramp time\n",
    "half_cycle = 2 \n",
    "t_approach = half_cycle*((approach - approach.min(axis=0)) / (approach.max(axis=0) - approach.min(axis=0)))\n",
    "t_withdraw = half_cycle*((withdraw - withdraw.max(axis=0)) / (withdraw.min(axis=0) - withdraw.max(axis=0)))+max(t_approach)\n",
    "t = np.concatenate([t_approach, t_withdraw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisr\\AppData\\Local\\Temp\\ipykernel_15088\\1284831555.py:4: RuntimeWarning: invalid value encountered in power\n",
      "  force = 4/3 * E / (1 - nu**2) * np.sqrt(r)*i**1.5 * factor\n",
      "C:\\Users\\luisr\\AppData\\Local\\Temp\\ipykernel_15088\\1284831555.py:18: RuntimeWarning: invalid value encountered in power\n",
      "  force = K * r **0.5 * i**1.5 - Ua * K**0.5 * r**0.75 * i**0.75\n"
     ]
    }
   ],
   "source": [
    "# construct dataframe\n",
    "df = pd.DataFrame()\n",
    "# 'E' and 'gamma' arrays to list:\n",
    "df['E'] = E.tolist()\n",
    "df['gamma'] = gamma.tolist()\n",
    "# assigns the displacement array for each 'E' (num of E values = len(df) = size)\n",
    "df['approach'] = [rnd_approach[app] for app in range(len(df))]\n",
    "df['withdraw'] = [rnd_withdraw[wd] for wd in range(len(df))]\n",
    "# '..._interp' columns have the sole purpose of allowing the sns errorbar plot \n",
    "df['approach_interp'] = [approach for _ in range(len(df))]\n",
    "df['withdraw_interp'] = [withdraw for _ in range(len(df))]\n",
    "# applies hertz and jkr models to each row (axis= 0(col) or 1(row))\n",
    "    # x will take the values of each row \n",
    "df['f_hertz'] = df.apply(lambda x: hertz(x.approach, x.E, nu, r), axis=1)\n",
    "df['f_jkr'] = df.apply(lambda x: jkr(x.withdraw, x.E, nu, x.gamma, r), axis=1)\n",
    "df['f_hertz_interp'] = df.apply(lambda x: np.interp(x.approach_interp, x.approach, x.f_hertz), axis=1)\n",
    "df['f_jkr_interp'] = df.apply(lambda x: np.interp(-x.withdraw_interp, -x.withdraw, x.f_jkr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (50,)\n",
      "Negative values in row 0\n"
     ]
    }
   ],
   "source": [
    "#dataframe with contact-only data\n",
    "#df_jkr: dataframe for jkr data\n",
    "df_jkr = pd.DataFrame()\n",
    "df_jkr['withdraw'] = df['withdraw'].copy()\n",
    "df_jkr['withdraw_contact'] = df_jkr['withdraw'].copy().apply(lambda x: x[x>0])\n",
    "df_jkr['f_jkr'] = df['f_jkr'].copy()\n",
    "df_jkr['f_jkr_contact'] = df_jkr['f_jkr'].copy().apply(lambda x: x[:-(len(no_contact))])\n",
    "df_jkr['E_jkr'] = df['E'].copy()\n",
    "df_jkr['gamma_jkr'] = df['gamma'].copy()\n",
    "\n",
    "#check size of disp and force vectors\n",
    "print(df_jkr['withdraw_contact'][0].shape, df_jkr['f_jkr_contact'][0].shape)\n",
    "for i in range(size):\n",
    "    # check if there are negative values in 'f_hertz_contact' \n",
    "    if (df_jkr['f_jkr_contact'][i]<0).any():\n",
    "        print('Negative values in row', i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(series: pd.Series):\n",
    "    return series.mean(), series.std()\n",
    "\n",
    "def get_max_min(series: pd.Series):\n",
    "    return series.max(), series.min()\n",
    "\n",
    "def normalize(x, min, max):\n",
    "    return (x-min)/(max-min)\n",
    "\n",
    "def unnormalize(x, min, max):\n",
    "    return min + x*(max-min)\n",
    "\n",
    "get_mean_std(df_jkr['E_jkr']), get_mean_std(df_jkr['gamma_jkr']), get_max_min(df_jkr['E_jkr']), get_max_min(df_jkr['gamma_jkr'])\n",
    "\n",
    "e_max, e_min = get_max_min(df_jkr['E_jkr'])\n",
    "gamma_max, gamma_min = get_max_min(df_jkr['gamma_jkr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "Split 2\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"while nbins >= 2:\\n    print(nbins)\\n    try:\\n        df_jkr_norm['E_jkr_cat'] = pd.cut(df_jkr_norm['E_jkr_norm'], bins=nbins)\\n        df_jkr_norm['gamma_jkr_cat'] = pd.cut(df_jkr_norm['gamma_jkr_norm'], bins=nbins)\\n        train_df_jkr_norm, test_df_jkr = train_test_split(df_jkr_norm, test_size=test_ratio, \\n                                             stratify=df_jkr_norm[target_cols], random_state=rnd_state_jkr)\\n        break\\n    except:\\n        nbins += -1\\n\\nwhile nbins >= 2:\\n    print(nbins)\\n    try:\\n        train_df_jkr, valid_df_jkr = train_test_split(train_df_jkr_norm, test_size=valid_ratio_jkr/(1-test_ratio_jkr), \\n                                             stratify=train_df_jkr_norm[target_cols], random_state=rnd_state_jkr)\\n        break\\n    except:\\n        nbins += -1\""
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ratio_jkr = 0.15\n",
    "valid_ratio_jkr = 0.15\n",
    "rnd_state_jkr = 42\n",
    "# SEE INTERSTRAT.ML_STRATIFIERS\n",
    "\n",
    "target_cols = ['E_jkr_cat', 'gamma_jkr_cat']\n",
    "nbins = 20\n",
    "\n",
    "print('Split 1')\n",
    "while nbins >= 1:\n",
    "    print(nbins)\n",
    "    try:\n",
    "        df_jkr['E_jkr_cat'] = pd.cut(df_jkr['E_jkr'], bins=nbins)\n",
    "        df_jkr['gamma_jkr_cat'] = pd.cut(df_jkr['gamma_jkr'], bins=nbins)\n",
    "        train_df_jkr, test_df_jkr = train_test_split(df_jkr, test_size=test_ratio_jkr, \n",
    "                                             stratify=df_jkr[target_cols], random_state=rnd_state_jkr)\n",
    "        break\n",
    "    except:\n",
    "        nbins += -1\n",
    "print('Split 2')\n",
    "nbins = 20\n",
    "while nbins >= 1:\n",
    "    print(nbins)\n",
    "    try:\n",
    "        train_df_jkr, valid_df_jkr = train_test_split(train_df_jkr, test_size=valid_ratio_jkr/(1-test_ratio_jkr), \n",
    "                                             stratify=train_df_jkr[target_cols], random_state=rnd_state_jkr)\n",
    "        break\n",
    "    except:\n",
    "        nbins += -1\n",
    "\n",
    "'''while nbins >= 2:\n",
    "    print(nbins)\n",
    "    try:\n",
    "        df_jkr_norm['E_jkr_cat'] = pd.cut(df_jkr_norm['E_jkr_norm'], bins=nbins)\n",
    "        df_jkr_norm['gamma_jkr_cat'] = pd.cut(df_jkr_norm['gamma_jkr_norm'], bins=nbins)\n",
    "        train_df_jkr_norm, test_df_jkr = train_test_split(df_jkr_norm, test_size=test_ratio, \n",
    "                                             stratify=df_jkr_norm[target_cols], random_state=rnd_state_jkr)\n",
    "        break\n",
    "    except:\n",
    "        nbins += -1\n",
    "\n",
    "while nbins >= 2:\n",
    "    print(nbins)\n",
    "    try:\n",
    "        train_df_jkr, valid_df_jkr = train_test_split(train_df_jkr_norm, test_size=valid_ratio_jkr/(1-test_ratio_jkr), \n",
    "                                             stratify=train_df_jkr_norm[target_cols], random_state=rnd_state_jkr)\n",
    "        break\n",
    "    except:\n",
    "        nbins += -1'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7000, 2), (7000, 2), (1500, 2), (1500, 2), (1500, 2), (1500, 2))"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jkr_df_list = [train_df_jkr, test_df_jkr, valid_df_jkr]\n",
    "ft_cols = ['withdraw_contact', 'f_jkr_contact']\n",
    "# lb_cols = ['E_jkr_norm', 'gamma_jkr_norm']\n",
    "lb_cols = ['E_jkr', 'gamma_jkr']\n",
    "# lb_cols = ['gamma_jkr', 'E_jkr']\n",
    "dataset_jkr_list = []\n",
    "\n",
    "for _, df in enumerate(jkr_df_list):\n",
    "    aux_arr_ft = np.array(df[ft_cols])\n",
    "    dataset_jkr_list.append(aux_arr_ft)\n",
    "    aux_arr_lb = np.array(df[lb_cols])\n",
    "    dataset_jkr_list.append(aux_arr_lb)\n",
    "\n",
    "x_train_jkr, y_train_jkr, x_test_jkr, y_test_jkr, x_valid_jkr, y_valid_jkr = dataset_jkr_list\n",
    "\n",
    "x_train_jkr.shape, y_train_jkr.shape, x_test_jkr.shape, y_test_jkr.shape, x_valid_jkr.shape, y_valid_jkr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([7000, 50, 2]),\n",
       " torch.Size([7000, 2]),\n",
       " torch.Size([1500, 2]),\n",
       " torch.Size([1500, 2]))"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and test data from np arrays to torch tensor with desired shape\n",
    "def tensor_input_shape(nparray):\n",
    "    '''\n",
    "    Input: nparray - numpy array with two dimensions (n_samples, n_features)\n",
    "    Output: torch_tensor - pytorch tensor with 3 dimensions (n_samples, n_pts, n_features) \n",
    "    '''\n",
    "    n_samples = len(nparray)\n",
    "    n_pts = len(nparray[0,0])\n",
    "    torch_tensor = torch.zeros(size=(n_samples, n_pts, 2))\n",
    "    for i in range(n_samples):\n",
    "        aux_nparray = np.hstack((np.array(nparray[i,0]).reshape((n_pts,1)), np.array(nparray[i,1]).reshape((n_pts,1))))\n",
    "        aux_ttensor = torch.from_numpy(aux_nparray).type(torch.float)\n",
    "        torch_tensor[i,:,:] = aux_ttensor\n",
    "    return torch_tensor\n",
    "\n",
    "print(x_train_jkr[1,1].shape)\n",
    "\n",
    "x_train_t_jkr = tensor_input_shape(x_train_jkr)\n",
    "x_valid_t_jkr = tensor_input_shape(x_valid_jkr)\n",
    "x_test_t_jkr = tensor_input_shape(x_test_jkr)\n",
    "y_train_t_jkr = torch.from_numpy(y_train_jkr).type(torch.float)\n",
    "y_valid_t_jkr = torch.from_numpy(y_valid_jkr).type(torch.float)\n",
    "y_test_t_jkr = torch.from_numpy(y_test_jkr).type(torch.float)\n",
    "#x_train_t2 = torch.from_numpy(x_train).type(torch.float)\n",
    "\n",
    "x_train_t_jkr.shape, y_train_t_jkr.shape, y_valid_t_jkr.shape, y_test_t_jkr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dir(timestamp, contact_model: str):\n",
    "\n",
    "  ''' Second input must be 'hertz' or 'jkr' '''\n",
    "  \n",
    "  allowed_models = ['hertz', 'jkr']\n",
    "  if contact_model not in allowed_models:\n",
    "    raise ValueError(\"Input value must be one of %s\" % allowed_models)\n",
    "  model_path = 'model_{}'.format(timestamp)\n",
    "  parent_dir = 'c:\\\\Users\\\\luisr\\\\OneDrive\\\\Ambiente de Trabalho\\\\Tese'\n",
    "  if contact_model == 'hertz':\n",
    "    dir = 'Hertz_models'\n",
    "  elif contact_model == 'jkr':\n",
    "    dir = 'JKR_models'\n",
    "  path = os.path.join(parent_dir, dir, model_path)\n",
    "  # path = os.path.join(initial_wd, dir, model_path)\n",
    "  os.mkdir(path)\n",
    "  os.chdir(path)\n",
    "\n",
    "def error_fn(predict_tensor, label_tensor):\n",
    "  '''\n",
    "  INPUTS: * two tensors - true labels and predicts\n",
    "  OUTPUTS: * scalar - mean relative error (in %) between both tensors\n",
    "           * list - relative error (%) for each prediction\n",
    "  '''\n",
    "  error = abs((label_tensor-predict_tensor)/label_tensor*100).squeeze(dim=1).mean().item()\n",
    "  error_list = list(abs((label_tensor-predict_tensor)/label_tensor*100).squeeze(dim=1).detach().numpy())\n",
    "  return error, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JKR_Dataset():\n",
    "  \n",
    "  def __init__(self,features,labels):\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    " \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "   \n",
    "  def __getitem__(self,idx):\n",
    "    return self.features[idx],self.labels[idx]\n",
    "  \n",
    "train_data_jkr = JKR_Dataset(x_train_t_jkr, y_train_t_jkr)\n",
    "test_data_jkr = JKR_Dataset(x_test_t_jkr, y_test_t_jkr)\n",
    "valid_data_jkr = JKR_Dataset(x_valid_t_jkr, y_valid_t_jkr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################ After changing one of the hyperparameters: ########################\n",
    "### Re-run the cells where the model class and the model_params dict are defined ###\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "LEARNING_RATE_JKR = 0.0005\n",
    "EPOCHS_JKR = 50\n",
    "BATCH_SIZE_JKR = 128\n",
    "\n",
    "# Size of each layer\n",
    "HIDDEN_UNITS_1_JKR = 512\n",
    "HIDDEN_UNITS_2_JKR = 256\n",
    "HIDDEN_UNITS_3_JKR = 64\n",
    "\n",
    "ARCHITECTURE_JKR = 2\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_jkr=DataLoader(train_data_jkr,batch_size=BATCH_SIZE_JKR,shuffle=True)\n",
    "test_loader_jkr=DataLoader(test_data_jkr,batch_size=int(test_ratio_jkr*size+1),shuffle=False)\n",
    "valid_loader_jkr=DataLoader(valid_data_jkr, batch_size=int(valid_ratio_jkr*size+1), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "        self.mape = nn.L1Loss(reduction='none')\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        mape = self.mape(input, target)\n",
    "        norm_factor = torch.abs(target)\n",
    "        mape = (mape / norm_factor)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression_JKR(nn.Module):\n",
    "    def __init__(self, input_shape, HIDDEN_UNITS_1_JKR, HIDDEN_UNITS_2_JKR, HIDDEN_UNITS_3_JKR):\n",
    "        super(Regression_JKR, self).__init__()\n",
    "        input_size = input_shape[0] * input_shape[1]\n",
    "        self.layers = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(input_size, HIDDEN_UNITS_1_JKR),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_1_JKR,HIDDEN_UNITS_2_JKR),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_2_JKR,HIDDEN_UNITS_3_JKR),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_3_JKR, 2))\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "# Define input shape\n",
    "input_shape_jkr = x_train_t_jkr.shape[1:]\n",
    "\n",
    "# Instantiate the model\n",
    "torch.manual_seed(42)\n",
    "model_jkr = Regression_JKR(input_shape_jkr, HIDDEN_UNITS_1_JKR, HIDDEN_UNITS_2_JKR, HIDDEN_UNITS_3_JKR)\n",
    "\n",
    "\n",
    "# Experimentar Huber Loss, log-cosh e quantile loss !!!!!!!!!!\n",
    "# Define the loss function and optimizer\n",
    "loss_fn_jkr = nn.MSELoss(reduction='none')\n",
    "# loss_fn_jkr = nn.MSELoss()\n",
    "# loss_fn_jkr = nn.L1Loss(reduction='none')\n",
    "\n",
    "optimizer_jkr = torch.optim.Adam(model_jkr.parameters(),\n",
    "                                lr=LEARNING_RATE_JKR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class Regression_JKR_norm(nn.Module):\\n    def __init__(self, input_shape, HIDDEN_UNITS_1_JKR, HIDDEN_UNITS_2_JKR, HIDDEN_UNITS_3_JKR):\\n        super(Regression_JKR_norm, self).__init__()\\n        input_size = input_shape[0] * input_shape[1]\\n        self.layers = nn.Sequential(nn.Flatten(),\\n                                    nn.Linear(input_size, HIDDEN_UNITS_1_JKR),\\n                                    nn.ReLU(),\\n                                    nn.Linear(HIDDEN_UNITS_1_JKR,HIDDEN_UNITS_2_JKR),\\n                                    nn.ReLU(),\\n                                    nn.Linear(HIDDEN_UNITS_2_JKR,HIDDEN_UNITS_3_JKR),\\n                                    nn.ReLU(),\\n                                    nn.Linear(HIDDEN_UNITS_3_JKR, 2),\\n                                    nn.BatchNorm1d(2))\\n    def forward(self, x):\\n        out = self.layers(x)\\n        return out\\n# Define input shape\\ninput_shape_jkr = x_train_t_jkr.shape[1:]\\n\\n# Instantiate the model\\ntorch.manual_seed(42)\\nmodel_jkr_norm = Regression_JKR_norm(input_shape_jkr, HIDDEN_UNITS_1_JKR, HIDDEN_UNITS_2_JKR, HIDDEN_UNITS_3_JKR)\\n\\n# Define the loss function and optimizer\\n# loss_fn_jkr_norm = nn.MSELoss(reduction='none')\\n# loss_fn_jkr_norm = nn.MSELoss()\\nloss_fn_jkr_norm = nn.MSELoss(reduction='none')\\n\\noptimizer_jkr_norm = torch.optim.SGD(model_jkr_norm.parameters(),\\n                                lr=LEARNING_RATE_JKR)\""
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Regression_JKR_norm(nn.Module):\n",
    "    def __init__(self, input_shape, HIDDEN_UNITS_1_JKR, HIDDEN_UNITS_2_JKR, HIDDEN_UNITS_3_JKR):\n",
    "        super(Regression_JKR_norm, self).__init__()\n",
    "        input_size = input_shape[0] * input_shape[1]\n",
    "        self.layers = nn.Sequential(nn.Flatten(),\n",
    "                                    nn.Linear(input_size, HIDDEN_UNITS_1_JKR),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_1_JKR,HIDDEN_UNITS_2_JKR),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_2_JKR,HIDDEN_UNITS_3_JKR),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(HIDDEN_UNITS_3_JKR, 2),\n",
    "                                    nn.BatchNorm1d(2))\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "# Define input shape\n",
    "input_shape_jkr = x_train_t_jkr.shape[1:]\n",
    "\n",
    "# Instantiate the model\n",
    "torch.manual_seed(42)\n",
    "model_jkr_norm = Regression_JKR_norm(input_shape_jkr, HIDDEN_UNITS_1_JKR, HIDDEN_UNITS_2_JKR, HIDDEN_UNITS_3_JKR)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# loss_fn_jkr_norm = nn.MSELoss(reduction='none')\n",
    "# loss_fn_jkr_norm = nn.MSELoss()\n",
    "loss_fn_jkr_norm = nn.MSELoss(reduction='none')\n",
    "\n",
    "optimizer_jkr_norm = torch.optim.SGD(model_jkr_norm.parameters(),\n",
    "                                lr=LEARNING_RATE_JKR)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_jkr(epoch_index, train_loader): # (epoch_index, tb_writer)\n",
    "    # running_loss = 0.\n",
    "    # last_loss = 0.\n",
    "    loss_list = []\n",
    "    error_E_list = []\n",
    "    error_gamma_list = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        optimizer_jkr.zero_grad()\n",
    "        predicts = model_jkr(inputs)\n",
    "        # print(predicts)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn_jkr(predicts, labels).mean(dim=0)\n",
    "        '''max, _ = loss_fn_jkr(predicts, labels).max(0)\n",
    "        loss = (loss_fn_jkr(predicts, labels)/max).mean()'''\n",
    "        '''loss1, loss2 = loss_fn_jkr(predicts[:,0], labels[:,0]), loss_fn_jkr(predicts[:,1], labels[:,1])\n",
    "        loss = loss1 * loss2'''\n",
    "        error_E, _ = error_fn(predicts[:,0].unsqueeze(dim=1), labels[:,0].unsqueeze(dim=1))\n",
    "        error_gamma, _ = error_fn(predicts[:,1].unsqueeze(dim=1), labels[:,1].unsqueeze(dim=1))\n",
    "        loss.backward(gradient=torch.Tensor([1., 1.]))\n",
    "        # Adjust learning weights\n",
    "        optimizer_jkr.step()\n",
    "        # Gather data and report\n",
    "        loss_list.append(loss.detach().numpy())\n",
    "        error_E_list.append(error_E)\n",
    "        error_gamma_list.append(error_gamma)\n",
    "        # running_loss += loss.item()  # .item() converts tensor to number\n",
    "        # print(i, loss.item())\n",
    "    return loss_list, error_E_list, error_gamma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    # n_layers = trial.suggest_int(\"n_layers\", 3, 4)\n",
    "    n_layers = 3\n",
    "    layers = []\n",
    "    neurons = []\n",
    "    in_features = input_shape_jkr[0] * input_shape_jkr[1]\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            layers.append(nn.Flatten())\n",
    "            out_features = 128\n",
    "            # out_features = trial.suggest_categorical(\"n_units_l{}\".format(i), [64, 128, 256])\n",
    "        elif i == 1 or i == 2:\n",
    "            out_features = 128\n",
    "        elif i == 2:\n",
    "            out_features = 64\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        neurons.append(out_features)\n",
    "        # p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        # layers.append(nn.Dropout(p))\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 2))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers), neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-15 15:25:15,169]\u001b[0m A new study created in memory with name: JKR_Huber_LeakyReLU\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:27:36,170]\u001b[0m Trial 0 finished with value: 0.01817111112177372 and parameters: {'learning_rate': 0.0007522660787915097}. Best is trial 0 with value: 0.01817111112177372.\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:29:49,782]\u001b[0m Trial 1 finished with value: 0.0023690471425652504 and parameters: {'learning_rate': 0.0008002728719926263}. Best is trial 1 with value: 0.0023690471425652504.\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:32:05,552]\u001b[0m Trial 2 finished with value: 0.005737136583775282 and parameters: {'learning_rate': 0.0022946667087656155}. Best is trial 1 with value: 0.0023690471425652504.\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:34:24,884]\u001b[0m Trial 3 finished with value: 0.007150169461965561 and parameters: {'learning_rate': 0.0004970912345515033}. Best is trial 1 with value: 0.0023690471425652504.\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:36:48,595]\u001b[0m Trial 4 finished with value: 0.02251722663640976 and parameters: {'learning_rate': 0.0002688385028753457}. Best is trial 1 with value: 0.0023690471425652504.\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:36:54,631]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:36:58,636]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:37:02,330]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:37:24,007]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:37:47,522]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:37:53,706]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:38:25,366]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:38:53,209]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:38:56,811]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:00,307]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:05,890]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:11,421]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:17,228]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:20,668]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:26,575]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:29,836]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:36,053]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:41,803]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:45,552]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:39:57,513]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:00,998]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:05,219]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:19,195]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:25,620]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:29,595]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:36,518]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:43,021]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:50,133]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:40:56,246]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:03,210]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:09,466]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:13,830]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:17,542]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:21,416]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:25,422]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:28,665]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:32,085]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:35,742]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:41:39,150]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:43:59,282]\u001b[0m Trial 44 finished with value: 0.0663917139172554 and parameters: {'learning_rate': 0.001391220225409266}. Best is trial 1 with value: 0.0023690471425652504.\u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:44:05,019]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:44:10,614]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:44:19,718]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:44:23,163]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:44:46,154]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:44:49,736]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:45:54,964]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:46:00,347]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:46:40,805]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:46:46,597]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:46:49,879]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:46:55,322]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:47:11,693]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:47:33,292]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:47:36,521]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:47:42,040]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:47:50,744]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:10,261]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:19,089]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:22,458]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:27,847]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:33,418]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:42,134]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:47,570]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:52,917]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:48:58,379]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:03,735]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:07,006]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:16,464]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:21,896]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:27,515]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:30,742]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:39,895]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:49:55,225]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:00,733]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:22,498]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:27,835]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:30,973]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:36,541]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:42,389]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:47,909]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:53,945]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:50:57,016]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:02,378]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:11,018]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:14,550]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:23,243]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:26,734]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:32,117]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:41,130]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:47,193]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:53,084]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:51:58,883]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:52:04,977]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2023-06-15 15:52:08,368]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  94\n",
      "  Number of complete trials:  6\n",
      "Best trial:\n",
      "  Value:  0.0023690471425652504\n",
      "  Params: \n",
      "    learning_rate: 0.0008002728719926263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_layers_list = []\n",
    "lr_list = []\n",
    "nl1_list, nl2_list, nl3_list = [], [], []\n",
    "epoch_list = []\n",
    "def objective(trial):\n",
    "\n",
    "    # model_Hertz = Regression_Hertz(input_shape, HIDDEN_UNITS_1, HIDDEN_UNITS_2).to(DEVICE)\n",
    "    model_JKR = define_model(trial)[0].to(DEVICE)\n",
    "    \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 2e-4, 5e-3, log=True)\n",
    "    # n_epochs = trial.suggest_int('n_epochs', 90, 140, step=10)\n",
    "    n_epochs = 120\n",
    "    # optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
    "    # batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "    batch_size = 16\n",
    "    # n_layers = trial.suggest_int(\"n_layers\", 3, 4)\n",
    "    # n_nodes = trial.suggest_int(\"n_nodes\", 4, 64, log=True)\n",
    "    train_loader_jkr=DataLoader(train_data_jkr,batch_size=batch_size,shuffle=True)\n",
    "    ## loss_fn = nn.HuberLoss()\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(model_JKR.parameters(), lr=learning_rate)\n",
    "    # loss_fn = MAPELoss()\n",
    "    # optimizer = getattr(optim, optimizer_name)(model_JKR.parameters(), lr=learning_rate)\n",
    "    epoch_list.append(n_epochs)\n",
    "    lr_list.append(learning_rate)\n",
    "    nl1_list.append(define_model(trial)[1][0])\n",
    "    nl2_list.append(define_model(trial)[1][1])\n",
    "    nl3_list.append(define_model(trial)[1][2])\n",
    "    for epoch in range(n_epochs):\n",
    "        model_JKR.train(True)\n",
    "        loss_list = []\n",
    "        error_list = []\n",
    "        for i, data in enumerate(train_loader_jkr):\n",
    "            # Every data instance is an input + label pair\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_JKR(inputs.to(DEVICE))\n",
    "            # print(f'Outputs: {outputs.mean()}')\n",
    "            # Compute the loss and its gradients\n",
    "            ## loss = loss_fn(outputs, labels.to(DEVICE)).mean()\n",
    "            loss = loss_fn(outputs, labels.to(DEVICE)).mean(dim=0)\n",
    "            # error, _ = error_fn(outputs, labels)\n",
    "            loss.backward(gradient=torch.Tensor([1., 1.]).to(DEVICE))\n",
    "            ##loss.backward()\n",
    "            ## loss_list.append(loss.item())\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "        # Evaluation\n",
    "        model_JKR.eval()\n",
    "        running_vloss = 0.0\n",
    "        # running_verror_E, running_verror_gamma = 0.0, 0.0\n",
    "        verror_list, fts_list, labels_list, predicts_list = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(valid_loader_jkr):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model_JKR(vinputs.to(DEVICE))\n",
    "                predicts_list.append(voutputs)\n",
    "                vloss = loss_fn(voutputs, vlabels.to(DEVICE)).mean()\n",
    "                # print(f'Loss: {vloss}')\n",
    "                # verror, verror_aux_list = error_fn(voutputs, vlabels)\n",
    "                # error_E, _ = error_fn(voutputs[:,0].unsqueeze(dim=1).to(DEVICE), vlabels[:,0].unsqueeze(dim=1).to(DEVICE))\n",
    "                # error_gamma, _ = error_fn(voutputs[:,1].unsqueeze(dim=1).to(DEVICE), vlabels[:,1].unsqueeze(dim=1).to(DEVICE))\n",
    "                running_vloss += vloss.item()\n",
    "                # running_verror_gamma += error_gamma\n",
    "                # running_verror_E += error_E\n",
    "                # verror_list += verror_aux_list\n",
    "            loss = running_vloss / (i + 1)\n",
    "            # verror_E = running_verror_E / (i + 1)\n",
    "            # verror_gamma = running_verror_gamma / (i + 1)\n",
    "            # avg_verror = ((verror_E + verror_gamma) / 2)\n",
    "            # print(loss.item())\n",
    "            # avg_verror = running_verror / (i + 1)\n",
    "            # print(avg_verror)\n",
    "        # trial.report(loss.item(), epoch)\n",
    "        trial.report(loss, epoch)\n",
    "        # trial.report(avg_verror, epoch)\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    # return loss\n",
    "    return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study_name = \"JKR_Huber_LeakyReLU\"\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=2), \n",
    "                                study_name=study_name)\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    losses = [trial.value for trial in study.trials]\n",
    "    losses_array = np.array(losses)\n",
    "    lr_array = np.array(lr_list)\n",
    "    # nl1_array = np.array(nl1_list)\n",
    "    # nl2_array = np.array(nl2_list)\n",
    "    # nl3_array = np.array(nl3_list)\n",
    "    # epoch_array = np.array(epoch_list)\n",
    "    \n",
    "    np.save(\"losses.npy\", losses_array)\n",
    "    np.save(\"lr.npy\", lr_array)\n",
    "    # np.save(\"nl1.npy\", nl1_array)\n",
    "    # np.save(\"nl2.npy\", nl2_array)\n",
    "    # np.save(\"nl3.npy\", nl3_array)\n",
    "    # np.save(\"epoch.npy\", epoch_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3], dtype=torch.float32, requires_grad=True)\n",
    "a.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
